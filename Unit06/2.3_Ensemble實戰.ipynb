{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3129ab3d",
   "metadata": {},
   "source": [
    "# Ensemble/Voting Classification in Python with Scikit-Learn\n",
    "ref：https://www.kaggle.com/c/titanic/submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6b020b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, log_loss\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7163b994",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_data = pd.read_csv(\"data/train.csv\")\n",
    "testing_data = pd.read_csv(\"data/test.csv\")\n",
    "def get_nulls(training, testing):\n",
    "    print(\"Training Data:\")\n",
    "    print(pd.isnull(training).sum())\n",
    "    print(\"Testing Data:\")\n",
    "    print(pd.isnull(testing).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffd6109f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:\n",
      "PassengerId    0\n",
      "Survived       0\n",
      "Pclass         0\n",
      "Sex            0\n",
      "Age            0\n",
      "SibSp          0\n",
      "Parch          0\n",
      "Fare           0\n",
      "Embarked       0\n",
      "dtype: int64\n",
      "Testing Data:\n",
      "PassengerId    0\n",
      "Pclass         0\n",
      "Sex            0\n",
      "Age            0\n",
      "SibSp          0\n",
      "Parch          0\n",
      "Fare           0\n",
      "Embarked       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Drop the cabin column, as there are too many missing values\n",
    "# Drop the ticket numbers too, as there are too many categories\n",
    "# Drop names as they won't really help predict survivors\n",
    "\n",
    "\n",
    "\n",
    "# Taking the mean/average value would be impacted by the skew\n",
    "# so we should use the median value to impute missing values\n",
    "training_data[\"Age\"].fillna(training_data[\"Age\"].median(),inplace=True)\n",
    "\n",
    "\n",
    "get_nulls(training_data, testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19e4f59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:\n",
      "PassengerId      0\n",
      "Survived         0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age            177\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             0\n",
      "Cabin          687\n",
      "Embarked         2\n",
      "dtype: int64\n",
      "Testing Data:\n",
      "PassengerId      0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age             86\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             1\n",
      "Cabin          327\n",
      "Embarked         0\n",
      "dtype: int64\n",
      "training_data: \n",
      "     PassengerId  Survived  Pclass     Sex   Age  SibSp  Parch     Fare  \\\n",
      "0              1         0       3    male  22.0      1      0   7.2500   \n",
      "1              2         1       1  female  38.0      1      0  71.2833   \n",
      "2              3         1       3  female  26.0      0      0   7.9250   \n",
      "3              4         1       1  female  35.0      1      0  53.1000   \n",
      "4              5         0       3    male  35.0      0      0   8.0500   \n",
      "..           ...       ...     ...     ...   ...    ...    ...      ...   \n",
      "886          887         0       2    male  27.0      0      0  13.0000   \n",
      "887          888         1       1  female  19.0      0      0  30.0000   \n",
      "888          889         0       3  female   NaN      1      2  23.4500   \n",
      "889          890         1       1    male  26.0      0      0  30.0000   \n",
      "890          891         0       3    male  32.0      0      0   7.7500   \n",
      "\n",
      "    Embarked  \n",
      "0          S  \n",
      "1          C  \n",
      "2          S  \n",
      "3          S  \n",
      "4          S  \n",
      "..       ...  \n",
      "886        S  \n",
      "887        S  \n",
      "888        S  \n",
      "889        C  \n",
      "890        Q  \n",
      "\n",
      "[891 rows x 9 columns]\n",
      "testing_data: \n",
      "     PassengerId  Pclass     Sex   Age  SibSp  Parch      Fare Embarked\n",
      "0            892       3    male  34.5      0      0    7.8292        Q\n",
      "1            893       3  female  47.0      1      0    7.0000        S\n",
      "2            894       2    male  62.0      0      0    9.6875        Q\n",
      "3            895       3    male  27.0      0      0    8.6625        S\n",
      "4            896       3  female  22.0      1      1   12.2875        S\n",
      "..           ...     ...     ...   ...    ...    ...       ...      ...\n",
      "413         1305       3    male   NaN      0      0    8.0500        S\n",
      "414         1306       1  female  39.0      0      0  108.9000        C\n",
      "415         1307       3    male  38.5      0      0    7.2500        S\n",
      "416         1308       3    male   NaN      0      0    8.0500        S\n",
      "417         1309       3    male   NaN      1      1   22.3583        C\n",
      "\n",
      "[418 rows x 8 columns]\n",
      "Training Data:\n",
      "PassengerId    0\n",
      "Survived       0\n",
      "Pclass         0\n",
      "Sex            0\n",
      "Age            0\n",
      "SibSp          0\n",
      "Parch          0\n",
      "Fare           0\n",
      "Embarked       0\n",
      "dtype: int64\n",
      "Testing Data:\n",
      "PassengerId    0\n",
      "Pclass         0\n",
      "Sex            0\n",
      "Age            0\n",
      "SibSp          0\n",
      "Parch          0\n",
      "Fare           0\n",
      "Embarked       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 先查缺漏值情況\n",
    "get_nulls(training_data, testing_data)\n",
    "\n",
    "# Drop the cabin column, as there are too many missing values (cabin缺漏太多,拿掉)\n",
    "# Drop the ticket numbers too, as there are too many categories(ticket太多類,拿掉)\n",
    "# Drop names as they won't really help predict survivors (names沒用,拿掉)\n",
    "training_data.drop(labels=[\"Cabin\",\"Ticket\",\"Name\"],axis=1, inplace=True)\n",
    "testing_data.drop(labels=[\"Cabin\",\"Ticket\",\"Name\"],axis=1, inplace=True)\n",
    "print(f\"training_data: \\n{training_data}\")\n",
    "print(f\"testing_data: \\n{testing_data}\")\n",
    "\n",
    "# 還是有很多缺漏值需要填補(Age用中位數來補值, Embarked填成\"S\"\n",
    "# Taking the mean/average value would be impacted by the skew\n",
    "# so we should use the median value to impute missing values\n",
    "# training_data[\"Age\"].fillna(training_data[\"Age\"].median(),inplace=True)# 舊方法未來可能有問題\n",
    "# testing_data[\"Age\"].fillna(training_data[\"Age\"].median(),inplace=True)# 舊方法未來可能有問題\n",
    "# training_data[\"Embarked\"].fillna(\"S\",inplace=True)# 舊方法未來可能有問題\n",
    "training_data[\"Age\"] = training_data[\"Age\"].fillna(training_data[\"Age\"].median()) # 直接覆蓋,不用inplace=True\n",
    "testing_data[\"Age\"] = testing_data[\"Age\"].fillna(training_data[\"Age\"].median())# 直接覆蓋,不用inplace=True\n",
    "training_data[\"Embarked\"] = training_data[\"Embarked\"].fillna(\"S\")# 直接覆蓋,不用inplace=True\n",
    "# (testing_data的Embarked沒有空值所以不必填)\n",
    "# testing_data[\"Fare\"].fillna(testing_data[\"Fare\"].median(),inplace=True) # 舊方法未來可能有問題\n",
    "testing_data[\"Fare\"] = testing_data[\"Fare\"].fillna(testing_data[\"Fare\"].median())# 直接覆蓋,不用inplace=True\n",
    "# (training_data的Fare沒有空值所以不必填)\n",
    "get_nulls(training_data, testing_data)\n",
    "\n",
    "\n",
    "\n",
    "# Fit the encoder on the data (Feature: Sex)\n",
    "encoder_Sex = LabelEncoder()\n",
    "encoder_Sex.fit(training_data[\"Sex\"])\n",
    "\n",
    "# Transform and replace training data\n",
    "training_sex_encoded = encoder_Sex.transform(training_data[\"Sex\"])\n",
    "training_data[\"Sex\"] = training_sex_encoded\n",
    "test_sex_encoded = encoder_Sex.transform(testing_data[\"Sex\"])\n",
    "testing_data[\"Sex\"] = test_sex_encoded\n",
    "# ----------------------------------------------------------\n",
    "# Fit the encoder on the data (Feature: Embarked)\n",
    "encoder_Embarked = LabelEncoder()\n",
    "encoder_Embarked.fit(training_data[\"Embarked\"])\n",
    "\n",
    "# Transform and replace training data\n",
    "training_Embarked_encoded = encoder_Embarked.transform(training_data[\"Embarked\"])\n",
    "training_data[\"Embarked\"] = training_Embarked_encoded\n",
    "test_Embarked_encoded = encoder_Embarked.transform(testing_data[\"Embarked\"])\n",
    "testing_data[\"Embarked\"] = test_Embarked_encoded\n",
    "\n",
    "# Any value we want to reshape needs be turned into array first\n",
    "Ages_train = np.array(training_data[\"Age\"]).reshape(-1, 1)\n",
    "Fare_train = np.array(training_data[\"Fare\"]).reshape(-1, 1)\n",
    "Ages_test = np.array(testing_data[\"Age\"]).reshape(-1, 1)\n",
    "Fare_test= np.array(testing_data[\"Fare\"]).reshape(-1, 1)\n",
    "\n",
    "\n",
    "# Scaler takes arrays\n",
    "scaler = StandardScaler()\n",
    "\n",
    "training_data[\"Age\"] = scaler.fit_transform(Ages_train)\n",
    "training_data[\"Fare\"] = scaler.fit_transform(Fare_train)\n",
    "testing_data[\"Age\"] = scaler.fit_transform(Ages_test)\n",
    "testing_data[\"Fare\"] = scaler.fit_transform(Fare_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65bd0090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Pclass  Sex       Age  SibSp  Parch      Fare  Embarked\n",
      "0       3    1 -0.565736      1      0 -0.502445         2\n",
      "1       1    0  0.663861      1      0  0.786845         0\n",
      "2       3    0 -0.258337      0      0 -0.488854         2\n",
      "3       1    0  0.433312      1      0  0.420730         2\n",
      "4       3    1  0.433312      0      0 -0.486337         2\n",
      "0    0\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    0\n",
      "Name: Survived, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Now to select our training/testing data\n",
    "X_features = training_data.drop(labels=['PassengerId', 'Survived'], axis=1)\n",
    "y_labels = training_data['Survived']\n",
    "\n",
    "print(X_features.head(5))\n",
    "print(y_labels.head(5))\n",
    "\n",
    "# Make the train/test data from validation\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_features, y_labels, test_size=0.1,random_state=12)\n",
    "\n",
    "\n",
    "#  ===========前處理到這邊完成==============================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5652bc34",
   "metadata": {},
   "source": [
    "## Simple Averaging Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cd452cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n"
     ]
    }
   ],
   "source": [
    "# Simple Averaging Approach\n",
    "# 假設要用3種方法\n",
    "LogReg_clf = LogisticRegression()\n",
    "DTree_clf = DecisionTreeClassifier()\n",
    "SVC_clf = SVC()\n",
    "\n",
    "LogReg_clf.fit(X_train, y_train)\n",
    "DTree_clf.fit(X_train, y_train)\n",
    "SVC_clf.fit(X_train, y_train)\n",
    "\n",
    "LogReg_pred = LogReg_clf.predict(X_val)\n",
    "DTree_pred = DTree_clf.predict(X_val)\n",
    "SVC_pred = SVC_clf.predict(X_val)\n",
    "\n",
    "averaged_preds = (LogReg_pred + DTree_pred + SVC_pred)//3\n",
    "acc = accuracy_score(y_val, averaged_preds)\n",
    "print(acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d32100",
   "metadata": {},
   "source": [
    "## Bagging Classification Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62ac47a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logreg_bagging_model score: 0.7927134146341464\n",
      "dtree_bagging_model score: 0.8188719512195123\n",
      "random_forest score: 0.8113719512195123\n",
      "extra_trees score: 0.7963719512195122\n"
     ]
    }
   ],
   "source": [
    "# Bagging Classification Example\n",
    "# base_estimator已棄用,改為estimator\n",
    "logreg_bagging_model = BaggingClassifier(estimator  = LogReg_clf,n_estimators=50,random_state=12)\n",
    "dtree_bagging_model = BaggingClassifier(estimator  = DTree_clf,n_estimators=50,random_state=12)\n",
    "random_forest = RandomForestClassifier(n_estimators=100,random_state=12)\n",
    "extra_trees = ExtraTreesClassifier(n_estimators=100,random_state=12)\n",
    "def bagging_ensemble(ModelName,model):\n",
    "    k_folds = KFold(n_splits=20, random_state=12,shuffle=True) # 分成20堆\n",
    "    results = cross_val_score(model, X_train, y_train, cv=k_folds)\n",
    "    print(f\"{ModelName} score: {results.mean()}\")\n",
    "\n",
    "\n",
    "bagging_ensemble(\"logreg_bagging_model\",logreg_bagging_model)\n",
    "bagging_ensemble(\"dtree_bagging_model\",dtree_bagging_model)\n",
    "bagging_ensemble(\"random_forest\",random_forest)\n",
    "bagging_ensemble(\"extra_trees\",extra_trees)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09094ea7",
   "metadata": {},
   "source": [
    "## Boosting Classification Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a6d48bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for 20 estimators:\n",
      "0.8064634146341463\n",
      "Results for 40 estimators:\n",
      "0.8089024390243903\n",
      "Results for 60 estimators:\n",
      "0.8051829268292684\n",
      "Results for 80 estimators:\n",
      "0.8039329268292683\n",
      "Results for 100 estimators:\n",
      "0.8051829268292684\n"
     ]
    }
   ],
   "source": [
    "# Boosting Classification Example\n",
    "k_folds = KFold(n_splits=20, random_state=12,shuffle=True) # 切成20堆\n",
    "num_estimators = [20, 40, 60, 80, 100] # 不一定越大越好\n",
    "\n",
    "for i in num_estimators:\n",
    "    ada_boost = AdaBoostClassifier(n_estimators=i,random_state=12)\n",
    "    results = cross_val_score(ada_boost,X_train,y_train,cv=k_folds)\n",
    "    print(\"Results for {} estimators:\".format(i))\n",
    "    print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8290c946",
   "metadata": {},
   "source": [
    "## voting\\Stacking Classification Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "161a4960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.8333333333333334\n",
      "Log Loss is: 6.0072755648528595\n",
      "F1 Score is: 0.7761194029850746\n",
      "====================================\n"
     ]
    }
   ],
   "source": [
    "# voting\\Stacking Classification Example\n",
    "# voting='hard' (多數決)\n",
    "voting_clf = VotingClassifier(estimators=[('SVC', SVC_clf), ('DTree', DTree_clf), ('LogReg', LogReg_clf)], voting='hard')\n",
    "voting_clf.fit(X_train, y_train)\n",
    "preds = voting_clf.predict(X_val)\n",
    "acc = accuracy_score(y_val, preds)\n",
    "l_loss = log_loss(y_val, preds)\n",
    "f1 = f1_score(y_val, preds)\n",
    "\n",
    "print(\"Accuracy is: \" + str(acc))\n",
    "print(\"Log Loss is: \" + str(l_loss))\n",
    "print(\"F1 Score is: \" + str(f1))\n",
    "\n",
    "print(\"====================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aedc530e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: \n",
      "     Pclass  Sex       Age  SibSp  Parch      Fare  Embarked\n",
      "715       3    1 -0.796286      0      0 -0.494391         2\n",
      "319       1    0  0.817561      1      1  2.059694         0\n",
      "829       1    0  2.508257      0      0  0.962353         2\n",
      "79        3    0  0.049062      0      0 -0.397241         2\n",
      "484       1    1 -0.335187      1      0  1.185430         0\n",
      "..      ...  ...       ...    ...    ...       ...       ...\n",
      "241       3    0 -0.104637      1      0 -0.336334         1\n",
      "253       3    1  0.049062      1      0 -0.324253         2\n",
      "390       1    1  0.510161      1      2  1.767741         2\n",
      "667       3    1 -0.104637      0      0 -0.491874         2\n",
      "843       3    1  0.394887      0      0 -0.518805         0\n",
      "\n",
      "[801 rows x 7 columns]\n",
      "testing_data: \n",
      "     PassengerId  Pclass  Sex       Age  SibSp  Parch      Fare  Embarked\n",
      "0            892       3    1  0.371062      0      0 -0.497413         1\n",
      "1            893       3    0  1.358985      1      0 -0.512278         2\n",
      "2            894       2    1  2.544493      0      0 -0.464100         1\n",
      "3            895       3    1 -0.221692      0      0 -0.482475         2\n",
      "4            896       3    0 -0.616861      1      1 -0.417492         2\n",
      "..           ...     ...  ...       ...    ...    ...       ...       ...\n",
      "413         1305       3    1 -0.142658      0      0 -0.493455         2\n",
      "414         1306       1    0  0.726714      0      0  1.314435         0\n",
      "415         1307       3    1  0.687197      0      0 -0.507796         2\n",
      "416         1308       3    1 -0.142658      0      0 -0.493455         2\n",
      "417         1309       3    1 -0.142658      1      1 -0.236957         0\n",
      "\n",
      "[418 rows x 8 columns]\n",
      "test.shape: (418, 7)\n",
      "submission_df: \n",
      "     PassengerId Survived\n",
      "0            892      NaN\n",
      "1            893      NaN\n",
      "2            894      NaN\n",
      "3            895      NaN\n",
      "4            896      NaN\n",
      "..           ...      ...\n",
      "413         1305      NaN\n",
      "414         1306      NaN\n",
      "415         1307      NaN\n",
      "416         1308      NaN\n",
      "417         1309      NaN\n",
      "\n",
      "[418 rows x 2 columns]\n",
      "submission_df: \n",
      "     PassengerId  Survived\n",
      "0            892         0\n",
      "1            893         0\n",
      "2            894         0\n",
      "3            895         0\n",
      "4            896         1\n",
      "..           ...       ...\n",
      "413         1305         0\n",
      "414         1306         1\n",
      "415         1307         0\n",
      "416         1308         0\n",
      "417         1309         0\n",
      "\n",
      "[418 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(f\"X_train: \\n{X_train}\")\n",
    "print(f\"testing_data: \\n{testing_data}\")\n",
    "\n",
    "# 先把PassengerId 弄掉\n",
    "test = testing_data.drop(labels=[\"PassengerId\"], axis=1)\n",
    "print(f\"test.shape: {test.shape}\")\n",
    "\n",
    "submission_df = pd.DataFrame(columns=[\"PassengerId\",\"Survived\"])\n",
    "submission_df[\"PassengerId\"] = testing_data[\"PassengerId\"]\n",
    "print(f\"submission_df: \\n{submission_df}\")\n",
    "Predict_Survived = voting_clf.predict(test)\n",
    "submission_df[\"Survived\"] = Predict_Survived\n",
    "print(f\"submission_df: \\n{submission_df}\")\n",
    "submission_df.to_csv(\"submissions.csv\",header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd282bc5-ef1f-418d-aad3-ac77b7a292da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
